---
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This dataset contains an anonymized set of variables, each representing a custom feature in a Mercedes vehicle. For example, a variable could be 4WD, added air suspension, or a head-up display. The ground truth is labeled 'y' and represents the time that the car needed to pass testing. The goal is to predict that time, with $R^2$ as the error metric.

# Exploratory Data Analysis and Feature Engineering

## General

Let's read in the data:
```{r read_in, message=FALSE, warning=FALSE}

# Requirements
require(dplyr)
require(ggplot2)
require(diagrammeR)
require(Matrix)
require(xgboost)
require(gridExtra)
require(reshape2)
require(ggcorrplot)
require(hclust)
require(doParallel)
require(mlr)
require(caret)
require(parallelMap)

df <- read.csv("train.csv")
```

From the description of the competition we know there's have an ID, a response, a bunch categorigal features, and a bigger bunch of binary features. Let's convert the categorical predictors to factors:

```{r missing}
df <- df %>%
  mutate_at(vars(X0, X1, X2, X3, X4, X5, X6, X8), as.factor)
```

A check for missing values:

```{r missing2}
any(is.na.data.frame(df))
```

No missing values. 

Let's see how many unique values we have in each feature:
```{r factors}
uniques <- apply(df, 2, function(x) length(unique(x))) 

data.frame(names = names(uniques), values = uniques) %>%
  filter(names != "y" & names != "ID") %>%
  ggplot(aes(x = 1:length(values), y = values)) + geom_line() + ggtitle('No. of unique values of features')
  
```

The majority of the features are binary (prone to overfitting), but there are also some features with just one value. Let's isolate and remove those, since they don't have any predictive power:

```{r factors2}
drop_cols <- apply(df, 2, function(x) length(unique(x)) < 2)

df <- df[, !drop_cols]
```

Alright, up to now we have an ID variable, a couple of categorical features (X0 - X8 as can be seen in the dataframe), and a very high number of binary features. Let's break down the features into these three subsets and take a look into them one-by-one:

## ID variable

I don't expect the ID of the vehicle to tell us that much, but I wouldn't be surprised if cars with a lower ID tend to spend more time on the test bench. Let's have a look:

```{r id_variable}
ggplot(data = df, aes(x = ID, y = y)) + geom_line() + ggtitle('ID vs testing time')
```

Indeed, no visible patterns. We can drop ID. Furthermore we can see that there is a very high response value (more than 260), let's clip that observation:

```{r id_variable2}
df <- df %>%
  select(-ID) %>%
  filter(y <= 250)
```

## Response variable

Let's have a closer look at the response:

```{r response_var, warning=FALSE}
p1 <- ggplot(df, aes(x = y)) + geom_histogram(binwidth = 3) + 
  ggtitle("Distribution of the response")

p2 <- df %>%
  filter(y <= 250) %>%
  ggplot(., aes(sample = y)) + stat_qq() + stat_qq_line() + ggtitle("Normal Q-Q plot of the response")

grid.arrange(p1, p2, nrow = 2)
```

Far from normal. We could apply a transformation at this point (log, sqrt, Box-Cox), we'll do so if it becomes necessary later on.

Alright, we need to do the following: (1) Find important features (2) identify interactions among predictors (if any) and (3) reduce dimensionality.

## Multi-valued factors

First, let's have a look at the few multi-valued factors X0 - X8, and how these affect the response. Starting with how many unique categories we have for each one of them:

```{r multi_factors_numbers, warning=FALSE}
factors <- c("X0", "X1", "X2", "X3", "X4", "X5", "X6", "X8")

uniques <- apply(df, 2, function(x) length(unique(x)))[2:9] 
  
data.frame(factors = factors, unique.vals = uniques) %>%
  ggplot(aes(x = factors, y = unique.vals)) + geom_bar(stat="identity") + ggtitle("No. of factors per categorical predictor")
```

Quite a few categories within each factor. A blind one-hot encoding might not be such a good idea.

Closer look at X0:

```{r multi_factors_x0_boxplot, fig.width=9, fig.height = 5, warning=FALSE}
# Box plot
ggplot(df, aes(x = X0, y = y)) + geom_boxplot() + ggtitle("Response per category - X0")
```

Categories AA (high response values) and AZ/BC (lowest response values) stand out. Furthermore, if we ignore outliers, we can see in general two types of responses for X0. A relatively low response (below 100) for multiple categories (AC, AD, AF and so on), and high response values (above 100) for the rest of them. Some exceptions exist of course, like category K. It might be possible to group all these categories into a few key super-categories.

AB, AC, G seem to have just one value. Let's confirm this:

```{r multi_factors_x0_some_vals}

for (elem in c("ab", "ac", "g"))
{
  print(paste(elem, ":", sum(df$X0 == elem)))
}
  
```

Let's see how many times each category appears in general:

```{r multi_factors_x0_counts, fig.width=9, fig.height=4}
df %>%
  count(X0) %>%
  ggplot(aes(x = X0, y = n)) + geom_bar(stat = "identity") + geom_abline(slope = 0, intercept = 20,  col = "red", lty = 2) + 
  ggtitle("Observations per category - X0")
```

There are quite a few categories that appear very infrequently - the red line indicates 20 appearances. We could goup them in a buffer "other" category, or merge them with the rest. Let's try clustering the observations, using median and interquantile range values:

```{r multi_factors_x0_clusters, fig.height=4, fig.width = 9}
# Get median and IQR
x0_lookup <- df %>%
  select(X0, y) %>%
  group_by(X0) %>%
  summarise(range = max(y) - min(y),
            median = median(y)) %>%
  ungroup %>%
  as.data.frame()
  
# Scale and center
x0.sum_scaled <- x0_lookup %>%
    select(-X0) %>%
    scale

# Try different k and plot elbow diagram
set.seed(25)

kappas <- 3:15

wss <- sapply(kappas, function(k){kmeans(x0.sum_scaled, k, nstart = 100, iter.max = 60)$tot.withinss})

data.frame(kappa = kappas, wss = wss) %>%
  ggplot(aes(x = kappa, y = wss)) + geom_line() + geom_point() + ggtitle("Elbow diagram - X0 category clustering") + 
  ylab("within-clusters sum of squares") + xlab("No. of clusters")
```

We can see that for a cluster size higher than 5, the between/total ss ratio starts decreasing at a slower rate. Let's visualize the results:

```{r multi_factors_x0_clusters_plot3d}
set.seed(25)

# Extract culster centers
x0.centers <- kmeans(x0.sum_scaled, centers = 5, nstart = 100)$centers

# Function to give the cluster for each variable
find_clusters <- function(x, centers) {
  # compute squared euclidean distance from each sample to each cluster center
  tmp <- sapply(seq_len(nrow(x)),
                function(i) apply(centers, 1,
                                  function(v) sum((x[i, ]-v)^2)))
  max.col(-t(tmp))  # find index of min distance
}

# Get the clusters
x0_lookup$clusters <- as.factor(apply(x0.sum_scaled, 1, function(x) find_clusters(t(as.vector(x)), x0.centers)) )

# Plot them
x0_lookup %>%
  ggplot(aes(x = range, y = median, color = clusters)) + geom_point() + ggtitle("X0 clustered categories") + 
  geom_text(aes(label = X0), nudge_x = 0, nudge_y = 5, size = 3)
```

AB, G, R (i.e. cluster 3) should be moved to cluster 5, and K to cluster 4. They will not change the distribution of the grouped categories, due to the relatively low number of observations they have:. Let's apply this and see how the clustered boxplot for X0 looks like:

```{r multi_factors_x0_transformed, warning=FALSE, fig.width=10, fig.height=6}
# Split values into high (H) and low (L) expected response regions
x0_lookup <- x0_lookup %>%
  select(X0, clusters) %>%
  setNames(., c("X0", "X0_new"))

# Append the new categories for X0
df <- plyr::join(df, x0_lookup, by = "X0") %>%
  mutate(X0_new = as.factor(X0_new))

# Box plots
p1 <- ggplot(df, aes(x = X0, y = y, color = X0_new)) + geom_boxplot() + ggtitle("X0 Clustered categories - Boxplots")

p2 <- ggplot(df, aes(x = X0_new, y = y)) + geom_boxplot()

grid.arrange(p1, p2, ncol = 1)
```

That looks better.

Moving on to X1:

```{r multi_factors_x1_boxplot, warning=FALSE}
ggplot(df, aes(x = X1, y = y)) + geom_boxplot() + ggtitle("Response per category - X1")
```

There doesn't seem to be that much variation in response among the different categories, apart from category Y. Distribution of values:

```{r multi_factors_x1_counts}
df %>%
  count(X1) %>%
  ggplot(aes(x = X1, y = n)) + geom_bar(stat = "identity")  + geom_abline(slope = 0, intercept = 20,  col = "red", lty = 2) + ggtitle("Observations per category - X1") 
```

A few categories seem to account for around 80% of the total training set. The only value for X1 that shows significant differences in the response is category "Y", but then again its only 20 observations having that value.

Let's try X2:

```{r multi_factors_x2_boxplot, fig.width = 9, fig.height = 5, warning=FALSE}
ggplot(df, aes(x = X2, y = y)) + geom_boxplot() + ggtitle("Response per category - X2")
```

We could group these categories as well, but first let's have a look at how many times each category appears:

```{r multi_factors_x2_counts, fig.height=4, fig.width = 9}
df %>%
  count(X2) %>%
  ggplot(aes(x = X2, y = n)) + geom_bar(stat = "identity")  + geom_abline(slope = 0, intercept = 60,  col = "red", lty = 2) +
  ggtitle("Observations per category - X2")
```

This is interesting. One dominant category (AS), 4 moderately occuring ones (AE, AI, AK, M), and then the rest. We could use a similar approach as for X0, but with such a low number of observations for the different categories, the criteria we'll be using for clustering are questionable. Let's make one buffer for all categories that appear less than 60 times, i.e. below the red line of the figure above and see how the new boxplot looks like:

```{r multi_factors_x2_transform_0, warning=FALSE, fig.width=9, fig.height=4}
# Split values into high (H) and low (L) expected response regions
x2_lookup <- df %>%
  select(X2, y) %>%
  count(X2) %>%
  mutate(X2 = as.character(X2)) %>%
  mutate(X2_new = ifelse(n <= 60, "other", X2)) %>%
  select(X2, X2_new) %>%
  mutate_all(funs(as.factor))

# Append the new categories for X2
df <- plyr::join(df, x2_lookup, by = "X2") %>%
  mutate(X2_new = as.factor(X2_new))

# Box plot
ggplot(df, aes(x = X2_new, y = y)) + geom_boxplot() + ggtitle("Grouped categories - X2")

```

We can combine the rest of the categories by eyeballing the boxplot:

* Cat. 1: AE, M, AI, F
* Cat. 2: AK, E, AS, other
* Cat. 3: AQ, R
* Cat. 4: S
* Cat. 5: N

```{r multi_factors_x2_transform_part_2, warning=FALSE, fig.width=9, fig.height=4}
df$X2 <- as.character(df$X2)
df$X2_new <- as.character(df$X2_new)
df[df$X2 %in% c("ae", "m", "ai", "f"), "X2_new"] <- 1
df[df$X2 %in% c("ak", "e", "as", "other"), "X2_new"] <- 2
df[df$X2 %in% c("aq", "r"), "X2_new"] <- 3
df[df$X2 == "s", "X2_new"] <- 4
df[df$X2 == "n", "X2_new"] <- 5

df <- df %>%
  mutate(X2 = as.factor(X2)) %>%
  mutate(X2_new = as.factor(X2_new))

# Box plot
ggplot(df, aes(x = X2_new, y = y)) + geom_boxplot() + ggtitle("Grouped categories - X2")
```

Next up, X3 X4, X6 - they all have a limited number of possible values:

```{r multi_factors_x346, warning=FALSE, fig.width=9}
p1 <- ggplot(df, aes(x = X3, y = y)) + geom_boxplot()

p2 <- ggplot(df, aes(x = X4, y = y)) + geom_boxplot()

p3 <- ggplot(df, aes(x = X6, y = y)) + geom_boxplot()

grid.arrange(p1, p2, p3, nrow=2, ncol=2)

```

None of them seems to present different responses with respect to distinct categories, X4 seems to be doing so, but that's probably beacuse of the limited occurences of different categories:

```{r multi_factors_x2_countsss, fig.height=4}
df %>%
  count(X4)
```

Indeed, just three occurences for categories different than d.

Moving on to X5:

```{r X5_check, warning=FALSE}
# Box plot
p1 <- ggplot(df, aes(x = X5, y = y)) + geom_boxplot() + ggtitle("Response and no. observations per category - X5")

p2 <- df %>%
  count(X5) %>%
  ggplot(aes(x = X5, y = n)) + geom_bar(stat = "identity")  + geom_abline(slope = 0, intercept = 60,  col = "red", lty = 2)

grid.arrange(p1, p2, nrow = 2)
```

No significant differences among categories with a significant number of observations.

This leaves us with X8:

```{r X8_check, warning=FALSE}
# Box plot
p1 <- ggplot(df, aes(x = X8, y = y)) + geom_boxplot() + ggtitle("Response and no. observations per category - X8")

p2 <- df %>%
  count(X8) %>%
  ggplot(aes(x = X8, y = n)) + geom_bar(stat = "identity")  + geom_abline(slope = 0, intercept = 60,  col = "red", lty = 2)

grid.arrange(p1, p2, nrow = 2)
```

Same picture as X3 and X6, no statistically significant differences in the response.

To sum this up, we've reduced the cardinality of X0 and X2, and we know that the rest of the categorical variables do not seem to have any significant predictive power.

## Binary predictors

Let's have a closer look at the binary features. If each binary predictor is a dummy variable indicating wheter or not a specific car has a feature (ordinal binary data), it would make sense that a higher number of features results in increased testing time. Let's have a look:


```{r bin_sum_up, warning=FALSE}
bin.sums <- df[, 10:365] %>% # Binary predictors only
  mutate_all(funs(as.numeric)) %>% # Sums don't work otherwise
  rowSums() %>%
  data.frame() %>%
  mutate(y = df$y) %>% # Add response 
  setNames(., c("Bin.Sum", "target"))

ggplot(bin.sums, aes(x = Bin.Sum, y = target)) + geom_point() + geom_smooth(method = "lm") + xlab("Sum of binary predictors") + ggtitle("Sum of binary predictors vs. target variable")
```

Honestly, I was expecting something better. Let's have a look at the one-to-zero ratio:

```{r binary_plots}
bin.cols <- 10:365

bin.df <- df[, bin.cols]

bin.features <- data.frame(feature = colnames(df[, bin.cols]), one_zero_ratio = apply(df[, bin.cols], 2, function(x) sum(as.numeric(x)) / nrow(df) * 100))

ggplot(bin.features, aes(x = 1:nrow(bin.features), y = one_zero_ratio)) + geom_bar(stat="identity") + 
  xlab("Binary feature ID") + ggtitle("One to zero balance of binary features - original") + 
  geom_abline(slope = 0, intercept = 5,  col = "red", lty = 2) + 
  geom_abline(slope = 0, intercept = 95,  col = "red", lty = 2)
```

The height of the columns is the number of unity values (as a percentage of all values), for each predictor. There are several predictors that have a percentage of unity or zero values close to 100%. Of course, these do not hold any predictive value. Let's clip the highly imbalanced proportions (imbalanced in this case means a ratio lying outside the 95/05 margin, given by the red lines):

```{r binary_plots_clipped}
bin.features.clipped <- bin.features %>%
  filter(one_zero_ratio < 95 & one_zero_ratio > 5)

ggplot(bin.features.clipped, aes(x = 1:nrow(bin.features.clipped), y = one_zero_ratio)) + geom_bar(stat="identity") + 
  xlab("Binary feature ID") + ggtitle("One to zero balance of binary features - clipped") 
```

We're left with 125 binary predictors. We need to identify if there are sets of features that are associated with each other. Given that we're dealing with binary data, the phi (mean square contingency) coefficient is suitable. Considering that the phi coefficient is just the Pearson correlation applied to dichotomous (binary) data, we can get away with just using the built-in cor() function:

```{r binary_data_corr}
# Remove imbalanced predictors from the dset
bin.cols <- which(colnames(bin.df) %in% bin.features.clipped$feature)

bin.df <- bin.df[, bin.cols]

# Make a correlation matirx
corr <- bin.df %>%
  mutate_all(funs(as.numeric)) %>%
  cor

# Plot
ggcorrplot(corr, type = "upper", show.diag = F, title = "Phi Coeffcients - Binary predictors") + 
  theme(axis.text.x = element_text(size = 3), axis.text.y = element_text(size = 3))
```

There is a bit of redundancy in there. Caret has a very convenient function that automatically reduces pair-wise correlations by searching a correlation matrix:

```{r binary_data_corr_cutoff}
# Get indices of redundant predictors
reduced.bin.predictors <- findCorrelation(abs(corr), cutoff = 0.7, names = F)

# Remove them from the list of binary columns
bin.df <- bin.df[, -c(reduced.bin.predictors)]

print(paste("Current no. of binary predictors:", ncol(bin.df)))
```

## Individual Feature importance 

Let's do one-hot encoding on the categorical features and then rank everything using XGBoost:

```{r xgb_importance}
# Reproducibility
set.seed(10)

# Keep relevant features only
dfNew <- bin.df %>%
  mutate(y = df$y) %>%
  mutate(X0 = df$X0_new) %>%
  mutate(X2 = df$X2_new)

get_feature_importance <- function(data)
{
  # Model matrix
  sparse_mtrx <- sparse.model.matrix(y~. -1, data)
  
  # XGBoost
  xgb <- xgboost(data = sparse_mtrx, 
                 label = data$y, 
                 booster = "dart",  
                 nthread = 4, 
                 nrounds = 250, 
                 verbose = 0)
  
  # Get feature importance
  important_features <- xgb.importance(feature_names = sparse_mtrx@Dimnames[[2]], model = xgb)
  
  return(list("mtrx" = sparse_mtrx, "mdl" = xgb, "feats" = important_features))
}

important_feats <- get_feature_importance(dfNew)

# Unpack
xgb <- important_feats$mdl
importance <- important_feats$feats
sparse_mtrx <- important_feats$sparse_mtrx

head(importance)
```

Interesting. Two features account for 60% of the gain. Let's visualize cumulative Gain:

```{r xgb_importance_cum_gain}
importance <- importance %>%
  mutate(Cumul.Gain = cumsum(Gain)) %>%
  select(Feature, Gain, Cumul.Gain)

p1 <- ggplot(importance, aes(x = 1:nrow(importance), y = Cumul.Gain)) + geom_line() + xlab("Feature ID") + 
  ggtitle("Cumulative Gain by feature")

p2 <- head(importance, 20) %>%
  ggplot(., aes(x = 1:nrow(.), y = Cumul.Gain)) + geom_line() + xlab("Feature ID") 

grid.arrange(p1, p2, nrow = 1)
```

Indeed, the first 4 features account for 70% of the gain, whereas the first 15 features account for roughly 80% of it. 

## Interaction terms

Let's have a look at interactions - if they exist at all:

```{r xgb_interaction_terms}
xgb.plot.tree(feature_names = colnames(sparse_mtrx), model = xgb, trees = 0:1)
```

We can see two-way interactions among the following pairs:

* X0 and X189
* X0 and X115

and three-way interaction among X0, X189 and X261. Let's add those as features:

```{r xgb_interaction_terms_add}
dfNew <- dfNew %>%
  mutate(X0_189 = X0:as.factor(X189)) %>%
  mutate(X0_115 = X0:as.factor(X115)) %>%
  mutate(X0_189_261 = X0:as.factor(X189):as.factor(X261))

str(dfNew[, c("X0_189", "X0_115", "X0_189_261")])
```

The three-way interaction does complicate the model. We've introduced 20 extra factor levels with the two-way interactions (which will result into 20 extra dimensions after the one-hot encoding), plus an extra 20 for the three-way interaction. For now, We'll move ahead with the two-way interactions only, and include the third one later on (if necessary):

```{r xgb_interaction_terms_delete}
dfNew <- dfNew %>%
  select(- X0_189_261)
```

Let's see the most common value pairs for the interaction terms:

```{r xgb_interaction_terms_table}
dfNew %>%
  select(X0_189, X0_115) %>%
  table()
```
There are some pairs that appear just once, twice or 5 times. There is a chance that these will be flagged as outliers later on, so let's see which observations these are:

```{r xgb_interaction_terms_observations_out}
unusual_interactions <- which((dfNew$X0_189 == "3:1" & dfNew$X0_115 == "3:0") | (dfNew$X0_189 == "4:0" & dfNew$X0_115 == "4:0") | (dfNew$X0_189 == "5:0" & dfNew$X0_115 == "5:0") | (dfNew$X0_189 == "5:1" & dfNew$X0_115 == "5:1"))

unusual_interactions
```

Let's apply a one-hot encoding at this point:

```{r one_hot_train_set}
# One-hot encoding of categorical features
dfNew_one_hot <- dfNew %>%
  model.matrix(y~.-1, .) %>%
  data.frame 
```


## Outlier Search

https://www.kaggle.com/msp48731/analysis-of-clusters-and-outliers


We'll use Hierarchical Agglomerate Clustering with complete linkage (which - in theory - merges outliers relatively late) for the purpose of outlier identification. As a distance metric, we'll be using Jaccard distance for out binary dataset:

```{r hierarchical_clustering, fig.height=4.5, fig.width=9, results='hide',fig.keep='all'}
# Get distance matrix
dist.mtrx <- dist(dfNew_one_hot, method = "binary")

# Compute clusters
h.clust <- hclust(dist.mtrx, method = "complete")

# Plot
plot(h.clust, labels = F, main = "Cluster dendrogram") + abline(h = 0.75, col = 'red')
```

There is no siginificant height decrease in any of the consecutive cuts, at least not the ones that are visible. On the other hand, there are some cuts that result in the formation of small clusters at a relatively high height, around 0.75 (red line in the graph above). Let's cut the dendrogram at that height and have a closer look at the number of observations per cluster:

```{r hierarchical_clusters_tree_cut_bins, fig.height=4.5, fig.width=9}
# Cut the dendrogram
h.clust.cut <- data.frame(cluster = cutree(h.clust, h = 0.75))

h.clust.cut %>%
  group_by(cluster) %>%
  summarize(n()) %>%
  data.frame %>%
  setNames(., c("cluster_id", "counts")) %>%
  ggplot(aes(x = cluster_id, y = counts)) + geom_bar(stat = "identity") +
  geom_text(aes(label = counts), hjust = -0.5, angle = 90) + 
  ylim(0, 500) + xlab("Cluster ID") + ylab("No. of observations") + ggtitle("Observations per cluster (height = .75)")
```

Indeed, there are clusters with a very limited number of observations in them (1 to 10). Let's isolate these observations and have a closer look:

```{r hierarchical_clusters_isolated_values}
small.clusters <- h.clust.cut %>%
  group_by(cluster) %>%
  summarize(n()) %>%
  data.frame %>%
  setNames(., c("cluster_id", "counts")) %>%
  filter(counts < 10) 

isolated.obs <- which(h.clust.cut$cluster %in% small.clusters$cluster_id)

isolated.obs
```

These are the observations that were clustered by themselves into three different clusters (cluster IDs: 44, 46, 47, 49).  Observations 425, 1521 and 2287 are possibly flagged as outlier due to their unusual interactions, as identified earlier. This leaves 11 'unusual' observations. Given a sample of more than 4200 observations, dropping 11 is not a big issue.

```{r drop_outliers}
outlrs <- isolated.obs[which(! isolated.obs %in% unusual_interactions)]
```


# Predictive Modelling

## Preprocessing

We need to wrap all the pre-processing steps we've done so far on the training set in a function and create our training/validation data:

```{r preprocess}

# Gather the reduced dataset
cols_to_use <- append(colnames(bin.df), # The reduced binary predictor subset identified earlier
                      values = c("X0", "X2"))
                              

run_preprocessing <- function(new_data, cols_keep = cols_to_use, x0_categories = x0_lookup, x2_categories = x2_lookup)
{
  # Keep necessary columns only
  df <- new_data[, cols_keep]
  
  # Append the new categories for X0
  df <- plyr::join(df, x0_categories, by = "X0") %>% 
    mutate(X0_new = as.factor(X0_new))
  
  # Append the new categories for X2
  df <- plyr::join(df, x2_categories, by = "X2") %>%
    mutate(X2_new = as.factor(X2_new)) 
  
  df$X2 <- as.character(df$X2)
  df$X2_new <- as.character(df$X2_new)
  df[df$X2 %in% c("ae", "m", "ai", "f"), "X2_new"] <- 1
  df[df$X2 %in% c("ak", "e", "as", "other"), "X2_new"] <- 2
  df[df$X2 %in% c("aq", "r"), "X2_new"] <- 3
  df[df$X2 == "s", "X2_new"] <- 4
  df[df$X2 == "n", "X2_new"] <- 5
  
  # Drop the non-important categorical predictors
  df <- df %>% 
    select(- c(X0, X2)) 
  
  # Add interaction terms
  df <- df %>%
    mutate(X0_189 = X0_new:as.factor(X189)) %>%
    mutate(X0_115 = X0_new:as.factor(X115)) 
  
  # One-hot
  dmy <-dummyVars(" ~ .", df) 
  
  df <- data.frame(predict(dmy, newdata = df))
  
  return(df)
}

```

Let's create the train and validation sets:

```{r split_set}
# Reproducibility
set.seed(150)

# Clean read train/test
data <- read.csv("train.csv")

df <- run_preprocessing(data) %>% 
  mutate(y = data$y) %>% # Append response
  filter(y <= 200) # Drop the one high-response observation

# Remove outliers
df <- df[-outlrs, ]

# Get feat. importance again (hust for consistency w.r.t. column names)
importance_df <- get_feature_importance(df)$feats

# Train/test split
trainIndex <- createDataPartition(df$y, p = .75, list = F)

train.set <- df[trainIndex, ]
test.set <- df[-trainIndex, ]

X.train <- subset(train.set, select = -c(y)) %>%
  as.matrix

y.train <- train.set %>%
  select(y) %>%
  as.matrix

X.test <- subset(test.set, select = -c(y)) %>%
  as.matrix

y.test <- test.set %>%
  select(y) %>%
  as.matrix

# Read prediction set
X.predict <- run_preprocessing(read.csv("test.csv"))
```

## XGBoost with varying number of features

Let's try a couple of XGBoost models with a smaller number of features, with respect to their importance. We'll try a different approach in this case, so as to avoid manual re-tuning everytime: Random Search. We need to define a number of functions, by making use of the mlr package:

* A function to return a dataset with the top N features (in terms of predictive power).
* A function to perform random search given the reduced dataset, number of iterations and parameter space, using a 10-fold CV process.

```{r xgb_training_functions}
# Function to return a dataset with a reduced number of features
reduce_feats <- function(data, no_feats, feat_list)
{
  # One-hot encoding
  dmy <-dummyVars(" ~ .", data) 
  
  # Get the top no_feats to use
  feats_to_use <- feat_list %>%
    select(Feature) %>%
    head(no_feats)
  
  # Generate reduced dataset
  new.df <- data.frame(predict(dmy, newdata = data)) %>%
    select(feats_to_use$Feature)
  
  return(new.df)
}


# Function to return the learning task given a number of features to use
define_learn_task <- function(no_features, X, y, feat_importance)
{
  # Gather the reduced dataset
  data <- reduce_feats(X, no_features, feat_importance) %>% 
    mutate(y = as.vector(y)) # Append target

  # Define target task
  ml_task <- makeRegrTask(data = data, target = "y")
  
  return(ml_task)
}


# Tune the model and return the results
tune_mdl <- function(no_feats, no_random_searches, parameters,  
                     X = X.train, y = y.train, feat_importance_list = importance_df)
{
  # Start a parallel pool
  parallelStartSocket(detectCores() - 1)
  
  # Define the learning task
  learning_task <- define_learn_task(no_feats, X, y, feat_importance_list)
  
  # Tune the model
  tuned_mdl <- tuneParams(learner = parameters$learner,
                         task = learning_task,
                         resampling = parameters$sampling_plan,
                         measures = rmse,
                         par.set = parameters$parameter_space,
                         control = makeTuneControlRandom(maxit = no_random_searches),
                         show.info = F)
  
  # Apply optimal parameters found earlier
  opt.mdl <- setHyperPars(learner = parameters$learner, par.vals = tuned_mdl$x)
  
  # Verify CV performance of the best model
  performance <- resample(learner = opt.mdl, 
                          task = learning_task, 
                          resampling = parameters$sampling_plan,
                          measures = list(rmse),
                          keep.pred = T)
  
  # Gather results
  results <- list("no_feats" = no_feats,
                  "opt_params" = tuned_mdl$x,
                  "cv.rmse" = performance$aggr,
                  "predictions" = performance$pred$data[, c("id", "truth", "response")])
  
  # Release cores
  parallelStop()
  
  return(results)
}
```

Time to run our iterations:

```{r xgb_training, warning=FALSE, results = 'hide'}
# Reproducibility
set.seed(135)

# Generate the necessary (constant) tuning parameters
param_list <- list(
  # Define search space
  "parameter_space" = makeParamSet( 
    makeIntegerParam("nrounds", lower = 10, upper = 200),
    makeIntegerParam("max_depth", lower = 1, upper = 10),
    makeNumericParam("eta", lower = .01, upper = .3),
    makeNumericParam("gamma", lower = 1, upper = 10),
    makeNumericParam("subsample", lower = 0.4, upper = 0.9),
    makeIntegerParam("min_child_weight",lower = 1,upper = 7),
    makeNumericParam("colsample_bytree",lower = 0.4,upper = 1)),
  # Define resampling plan
  "sampling_plan" = makeResampleDesc("CV", iters = 10),
  # Define the learner
  "learner" = makeLearner(cl = "regr.xgboost"))

xgb_mdls <- sapply(c(2, 5, 10, 20, 30, 50, 75, 98), function(features) tune_mdl(features, no_random_searches = 350, param_list))
```

Let's have a look at the results:

```{r xgb_result_post_processing, warning=FALSE}
data.frame("features" = unlist(xgb_mdls["no_feats", ]),
           "cv.rmse" = unlist(xgb_mdls["cv.rmse", ]))
```

This is interesting. It doesn't seem to matter that much whether we use 5 or 98 features. This can be explained by the feature importance identified earlier on. Cumulative gains by adding the extra features can be seen below:

```{r modelling_gains}
data.frame("no_feats" = 1:nrow(importance_df),
           "cum_gain" = cumsum(importance_df$Gain)) %>%
  ggplot(aes(x = no_feats, y = cum_gain)) + geom_line() +
  geom_point(data = data.frame("no_feats" = c(2, 5, 10, 20, 30, 50, 75, 98),
                               "cum_gain" = cumsum(importance_df$Gain)[c(2, 5, 10, 20, 30, 50, 75, 92)]),
             aes(x = no_feats, y = cum_gain)) + ggtitle("Cumulative gain of total features used in each model")
```

As such, we should obviously opt for the simplest model that gives low error, i.e. the one trained with the top 5 predictors. The optimal parameters for that model are the following:

```{r top_model_params}
xgb_mdls["opt_params", which(xgb_mdls["no_feats", ] == 2)] %>% unlist
```

## Test set performance

Let's train the best model over the entire training set and get a realistic performance on the test set:

```{r tune_xgb_test_set}
# Reproducibility
set.seed(18)

# Create Dmatrices
train_final <- reduce_feats(X.train, 5, importance_df) %>% as.matrix
test_final <- reduce_feats(X.test, 5, importance_df) %>% as.matrix

dtrain <- xgb.DMatrix(data = train_final, label = y.train)
dtest <- xgb.DMatrix(data = test_final, label = y.test)

# Make the final model for the test set predictions
xgb.final <- xgb.train(params = list(eta = 0.2979364,
                                   gamma = 8.6622699,
                                   max_depth = 2,
                                   colsample_bytree = 0.4182624,
                                   subsample = 0.6158664,
                                   min_child_weight = 5),
                       data = dtrain,
                       nrounds = 23,
                       watchlist = list(train = dtrain, test = dtest),
                       metric = list("r.sq", "rmse"),
                       verbose = 1,
                       print_every_n = 23)
```

The error difference between train and test sets is very small, which means we don't have an overfitting problem. Finally, we are getting an RMSE of 8.36 on the test set, using only 5 features!

## Submission

We'll train the model over the entire training set, and make the submission file:

```{r submission}
# Reproducibility
set.seed(18)

# Create Dmatrices
X <- rbind(train_final, test_final)

y <- rbind(y.train, y.test)

dtrain <- xgb.DMatrix(data = X, label = y)

xgb.final <- xgb.train(params = list(eta = 0.2979364,
                                   gamma = 8.6622699,
                                   max_depth = 2,
                                   colsample_bytree = 0.4182624,
                                   subsample = 0.6158664,
                                   min_child_weight = 5),
                     data = dtrain,
                     nrounds = 23,
                     metric = "rmse",
                     verbose = 0,
                     print_every_n = 23)

prediction.set <- reduce_feats(X.predict, 5, importance_df) %>% as.matrix

read.csv("test.csv") %>%
  select(ID) %>%
  mutate(y = predict(xgb.final, prediction.set)) %>%
  write.csv(., "submission.csv", row.names = F)
```

## Results

We're getting a score ($R^2$) of 0.53244 on the public leaderboard and 0.51768 on the private leaderboard (see pics below). Top three solutions on the private leaderboard are 0.55550, 0.55498 and 0.55450 respectively. 

On the whole, not bad for a model that needs only 5 out of the original 377 features... 

![](final_score.png)
![Final Score](final_score_0.png)






